{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e0e73fb",
   "metadata": {},
   "source": [
    "## Image Classification Using PCA and Gaussian Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03915902",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b809a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.linear_model import Perceptron\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import svds, LinearOperator\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2ff8b",
   "metadata": {},
   "source": [
    "### Initialize necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3391df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat_v73(file_name):\n",
    "    \"\"\"\n",
    "    Loads MATLAB v7.3 format file and returns the data as a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name: Name of the MATLAB file to load.\n",
    "\n",
    "    Returns:\n",
    "    - data: Dictionary containing the data from the MATLAB file.\n",
    "    \"\"\"\n",
    "\n",
    "    with h5py.File(file_name, 'r') as f:\n",
    "        data = {key: np.array(f[key]) for key in f.keys()}\n",
    "    return data\n",
    "\n",
    "\n",
    "def PCA_sparse(X, d):\n",
    "    \"\"\"\n",
    "    Performs dimensionality reduction using Sparse Principal Component Analysis (PCA) on a data matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Data matrix where each row represents an observation and each column represents a feature.\n",
    "    - d: The desired reduced dimension.\n",
    "\n",
    "    Returns:\n",
    "    - Y: The dimensionality-reduced data matrix.\n",
    "    - eigVector: The eigenvectors corresponding to the principal components.\n",
    "    - eigValue: The eigenvalues associated with the principal components.\n",
    "    \"\"\"\n",
    "\n",
    "    X = sparse.csr_matrix(X) # convert X to sparse matrix if it is not already\n",
    "\n",
    "\n",
    "    # eigenvalue analysis\n",
    "    X_mean = X.mean(axis=0)  # calculate mean\n",
    "    X = X - X_mean  # centering\n",
    "\n",
    "    def matvec(v):\n",
    "        v = v.reshape(-1, 1)  # reshape the input vector\n",
    "        return (X.T @ (X @ v)).ravel()  # ravel to make sure the output is 1D\n",
    "    \n",
    "    # Since our operator is symmetric (i.e., X.T @ X equals its transpose), rmatvec is the same as matvec\n",
    "    rmatvec = matvec\n",
    "    \n",
    "    Sx_op = LinearOperator((X.shape[1], X.shape[1]), matvec=matvec, rmatvec=rmatvec)\n",
    "\n",
    "    # Adjust this line to receive the singular values and the left and right singular vectors\n",
    "    U, eigValue, Vt = svds(Sx_op, k=d)  # calculate eigenvalues and eigenvectors\n",
    "\n",
    "    IX = np.argsort(eigValue)[::-1]  # sort eigenvalues in descending order and get indices\n",
    "    eigValue = eigValue[IX]\n",
    "    eigVector = Vt.T[:, IX]  # Use the right singular vectors (transpose of Vt) as eigenvectors\n",
    "\n",
    "    # normalization\n",
    "    norm_eigVector = np.sqrt(np.sum(eigVector**2, axis=0))\n",
    "    eigVector = eigVector / norm_eigVector\n",
    "\n",
    "    Y = X @ eigVector  # sparse matrix multiplication\n",
    "\n",
    "    return Y, eigVector, eigValue\n",
    "\n",
    "\n",
    "def sparse_distance_matrix(X, num_neighbors=5):\n",
    "    \"\"\"\n",
    "    Computes a sparse pair-wise distance matrix based on the data matrix using nearest neighbors.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Data matrix where each row represents an observation and each column represents a feature.\n",
    "    - num_neighbors: Number of nearest neighbors to consider, not including self.\n",
    "\n",
    "    Returns:\n",
    "    - D: Sparse pair-wise distance matrix.\n",
    "    \"\"\"\n",
    "    # Ensure X is a sparse matrix\n",
    "    X = sparse.csr_matrix(X)\n",
    "\n",
    "    # Use sklearn's NearestNeighbors to find nearest neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=num_neighbors+1, metric='manhattan')\n",
    "    knn.fit(X)\n",
    "    \n",
    "    # Get the distances and indices of the nearest neighbors\n",
    "    distances, indices = knn.kneighbors(X)\n",
    "    \n",
    "    # Build the sparse distance matrix\n",
    "    m, _ = X.shape\n",
    "    D = sparse.lil_matrix((m, m))\n",
    "    \n",
    "    for i in range(m):\n",
    "        D[i, indices[i]] = distances[i]\n",
    "\n",
    "    return D.tocsr()\n",
    "\n",
    "def kernel(X, kernel_type, para):\n",
    "    \"\"\"\n",
    "    Computes a sparse kernel matrix based on the data matrix and specified kernel type.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Data matrix where each row represents an observation and each column represents a feature.\n",
    "    - kernel_type: Type of kernel, can be 'simple', 'poly', or 'gaussian'.\n",
    "    - para: Parameter for computing the 'poly' kernel. For 'simple' and 'gaussian', it will be ignored.\n",
    "\n",
    "    Returns:\n",
    "    - K: Sparse kernel matrix.\n",
    "    \"\"\"\n",
    "    # Ensure X is a sparse matrix\n",
    "    X = csr_matrix(X)\n",
    "\n",
    "    if kernel_type == 'simple':\n",
    "        K = X.dot(X.transpose())\n",
    "    elif kernel_type == 'poly':\n",
    "        K = X.dot(X.transpose()) + 1\n",
    "        K = K.power(para)\n",
    "    elif kernel_type == 'gaussian':\n",
    "        D = sparse_distance_matrix(X)\n",
    "        D.data **= 2  # square the distances in-place\n",
    "        K = sparse.csr_matrix(np.exp(-D.toarray() / (2 * para**2)))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel_type: {kernel_type}\")\n",
    "    \n",
    "    return K\n",
    "\n",
    "def sparse_distance_matrix_new_data(X, Y=None, num_neighbors=5):\n",
    "    \"\"\"\n",
    "    Computes a sparse pair-wise distance matrix between X and Y using nearest neighbors.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Data matrix where each row represents an observation and each column represents a feature.\n",
    "    - Y: New data matrix.\n",
    "    - num_neighbors: Number of nearest neighbors to consider, not including self.\n",
    "\n",
    "    Returns:\n",
    "    - D: Sparse pair-wise distance matrix.\n",
    "    \"\"\"\n",
    "    # Ensure X and Y are sparse matrices\n",
    "    X = sparse.csr_matrix(X)\n",
    "    Y = sparse.csr_matrix(Y)\n",
    "\n",
    "    # Use sklearn's NearestNeighbors to find nearest neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=num_neighbors+1, metric='manhattan')\n",
    "    knn.fit(X)\n",
    "\n",
    "    # Get the distances and indices of the nearest neighbors\n",
    "    distances, indices = knn.kneighbors(Y)\n",
    "\n",
    "    # Build the sparse distance matrix\n",
    "    m, _ = Y.shape\n",
    "    n, _ = X.shape\n",
    "    D = sparse.lil_matrix((m, n))\n",
    "\n",
    "    for i in range(m):\n",
    "        D[i, indices[i]] = distances[i]\n",
    "\n",
    "    return D.tocsr()\n",
    "\n",
    "def kernel_new_data(Y, X, kernel_type, para):\n",
    "    \"\"\"\n",
    "    Computes a sparse kernel matrix between new data Y and training data X based on the specified kernel type.\n",
    "\n",
    "    Parameters:\n",
    "    - Y: New data matrix.\n",
    "    - X: Training data matrix where each row represents an observation and each column represents a feature.\n",
    "    - kernel_type: Type of kernel, can be 'simple', 'poly', or 'gaussian'.\n",
    "    - para: Parameter for computing the 'poly' kernel. For 'simple' and 'gaussian', it will be ignored.\n",
    "\n",
    "    Returns:\n",
    "    - K: Sparse kernel matrix.\n",
    "    \"\"\"\n",
    "    # Ensure X and Y are sparse matrices\n",
    "    X = sparse.csr_matrix(X)\n",
    "    Y = sparse.csr_matrix(Y)\n",
    "\n",
    "    if kernel_type == 'simple':\n",
    "        K = Y.dot(X.transpose())\n",
    "    elif kernel_type == 'poly':\n",
    "        K = Y.dot(X.transpose()) + 1\n",
    "        K = K.power(para)\n",
    "    elif kernel_type == 'gaussian':\n",
    "        D = sparse_distance_matrix_new_data(X, Y)\n",
    "        D.data **= 2  # square the distances in-place\n",
    "        D_dense = D.toarray()  # Convert sparse matrix to dense array\n",
    "        K = np.exp(-D_dense / (2 * para**2))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel_type: {kernel_type}\")\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "def compute_sigma(X, num_neighbors=5):\n",
    "    \"\"\"\n",
    "    Computes the value of sigma used in the Gaussian kernel based on the mean distance to the nearest neighbors.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Data matrix where each row represents an observation and each column represents a feature.\n",
    "    - num_neighbors: Number of nearest neighbors to consider, not including self.\n",
    "\n",
    "    Returns:\n",
    "    - sigma: Value of sigma for the Gaussian kernel.\n",
    "    \"\"\"\n",
    "    knn = NearestNeighbors(n_neighbors=num_neighbors+1, metric='euclidean')\n",
    "    knn.fit(X)\n",
    "    distances, _ = knn.kneighbors(X)\n",
    "\n",
    "    # Compute the mean of the distances to the nearest neighbors\n",
    "    mean_d_NN = np.mean(distances[:, 1:])\n",
    "\n",
    "    # Compute Ïƒ based on the formula\n",
    "    sigma = 5 * mean_d_NN\n",
    "\n",
    "    return sigma\n",
    "\n",
    "def kpca(X, d, kernel_type, para):\n",
    "    \"\"\"\n",
    "    Performs Kernel Principal Component Analysis (KPCA) on a data matrix X.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Data matrix where each row represents an observation and each column represents a feature.\n",
    "    - d: Reduced dimension.\n",
    "    - kernel_type: Type of kernel, can be 'simple', 'poly', or 'gaussian'.\n",
    "    - para: Parameter for computing the 'poly' and 'gaussian' kernel. For 'simple', it will be ignored.\n",
    "\n",
    "    Returns:\n",
    "    - Y: Dimensionality-reduced data.\n",
    "    - eigVector: Eigen-vectors, which can be used for pre-image reconstruction.\n",
    "    - eigValue: Eigenvalues.\n",
    "    \"\"\"\n",
    "    \n",
    "    if kernel_type not in ['simple', 'poly', 'gaussian']:\n",
    "        print(f\"\\nError: Kernel type {kernel_type} is not supported. \\n\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if kernel_type == 'gaussian':\n",
    "        sigma = compute_sigma(X)\n",
    "        #sigma =para\n",
    "        para = sigma\n",
    "\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # Compute the kernel matrix\n",
    "    K0 = kernel(X, kernel_type, para).toarray()  # convert to dense matrix\n",
    "    oneN = np.ones((N, N)) / N\n",
    "    K = K0 - oneN.dot(K0) - K0.dot(oneN) + oneN.dot(K0).dot(oneN)\n",
    "\n",
    "\n",
    "    # Perform eigenvalue analysis\n",
    "    eigValue, eigVector = eigs(K / N, k=d, which='LM')\n",
    "    eigValue = eigValue.real\n",
    "    eigVector = eigVector.real\n",
    "\n",
    "    # Normalize the eigenvectors\n",
    "    norm_eigVector = np.sqrt(np.sum(eigVector ** 2, axis=0))\n",
    "    eigVector = eigVector / norm_eigVector\n",
    "\n",
    "    # Perform dimensionality reduction\n",
    "    Y = K0.dot(eigVector)\n",
    "\n",
    "    return Y, eigVector, eigValue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac897d4",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction and Classification using PCA and Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e200048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing standard PCA...\n",
      "error rate of PCA on training data: 0.0980392156862745\n",
      "error rate of PCA on testing data: 0.23076923076923073\n",
      "\n",
      "\n",
      "Performing Kernel PCA...\n",
      "error rate of kPCA on training data: 0.38235294117647056\n",
      "error rate of kPCA on testing data: 0.42307692307692313\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load the dataset\n",
    "    data = loadmat_v73('./Dataset/YaleFaceData.mat')\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_x, train_t, test_x, test_t = data['train_x'], data['train_t'], data['test_x'], data['test_t']\n",
    "\n",
    "    d = 9  # Initialize the reduced dimensions\n",
    "\n",
    "    # Standard PCA\n",
    "    print('Performing standard PCA...')\n",
    "\n",
    "    # Perform PCA on the training data\n",
    "    train_PCA, eigVector, _ = PCA_sparse(train_x.T, d)\n",
    "    eigVector = eigVector[:, :d]\n",
    "\n",
    "    # Transform the test data using the PCA eigenvectors\n",
    "    test_PCA = np.dot(test_x.T, eigVector)\n",
    "    \n",
    "    # Classification using Perceptron\n",
    "    classifier = Perceptron()\n",
    "    classifier.fit(train_PCA, train_t.ravel())\n",
    "\n",
    "    # Calculate error rates for PCA on training and testing data\n",
    "    error_PCA_train = zero_one_loss(train_t.T.ravel(), classifier.predict(train_PCA))\n",
    "    error_PCA_test = zero_one_loss(test_t.T.ravel(), classifier.predict(test_PCA))\n",
    "\n",
    "    print(f'error rate of PCA on training data: {error_PCA_train}')\n",
    "    print(f'error rate of PCA on testing data: {error_PCA_test}')\n",
    "    print('\\n')\n",
    "\n",
    "    # kPCA\n",
    "    print('Performing Kernel PCA...')\n",
    "\n",
    "    # Perform Kernel PCA on the training data\n",
    "    train_kPCA, eigVector_kPCA, _ = kpca(train_x.T, d, 'gaussian', para=22546)\n",
    "    eigVector_kPCA = eigVector_kPCA[:, :d]\n",
    "\n",
    "    # Transform the test data using the Kernel PCA eigenvectors\n",
    "    test_kPCA = kernel_new_data(test_x.T, train_x.T, 'gaussian', para=22546).dot(eigVector_kPCA)\n",
    "\n",
    "    # Classification using Perceptron\n",
    "    classifier_kPCA = Perceptron()\n",
    "    classifier_kPCA.fit(train_kPCA, train_t.ravel())\n",
    "\n",
    "    # Calculate error rates for kPCA on training and testing data\n",
    "    error_kPCA_train = zero_one_loss(train_t.T.ravel(), classifier_kPCA.predict(train_kPCA))\n",
    "    error_kPCA_test = zero_one_loss(test_t.T.ravel(), classifier_kPCA.predict(test_kPCA))\n",
    "\n",
    "    print(f'error rate of kPCA on training data: {error_kPCA_train}')\n",
    "    print(f'error rate of kPCA on testing data: {error_kPCA_test}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
